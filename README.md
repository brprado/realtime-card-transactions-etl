# Pipeline de Transa√ß√µes Banc√°rias - Real-time Processing ETL

## üìã Objetivo do Projeto

Este projeto implementa um pipeline ETL completo para processamento em tempo real de transa√ß√µes banc√°rias, utilizando uma arquitetura moderna baseada em **Medallion Architecture** (Bronze, Silver, Gold). O sistema √© capaz de:

- **Ingest√£o em tempo real**: Captura de transa√ß√µes banc√°rias via Kafka
- **Processamento distribu√≠do**: Transforma√ß√µes usando Apache Spark (Streaming e Batch)
- **Orquestra√ß√£o**: Automa√ß√£o e agendamento com Apache Airflow
- **Detec√ß√£o de fraude**: Identifica√ß√£o de anomalias e padr√µes suspeitos
- **Analytics**: Agrega√ß√µes e m√©tricas de neg√≥cio para an√°lise

## üèóÔ∏è Arquitetura

O pipeline segue o padr√£o Medallion Architecture:

```
Producer (Faker) ‚Üí Kafka ‚Üí Bronze (Raw) ‚Üí Silver (Cleaned) ‚Üí Gold (Aggregated)
                                                                    ‚Üì
                                                              Dashboard/API
```

### Camadas de Dados

- **Bronze**: Dados brutos ingeridos do Kafka, armazenados em formato Delta Lake no MinIO (bucket `bronze`)
- **Silver**: Dados limpos, validados e enriquecidos com features para detec√ß√£o de fraude, armazenados em Delta Lake no MinIO (bucket `silver`)
- **Gold**: Agrega√ß√µes de neg√≥cio, m√©tricas e KPIs prontos para consumo, armazenados em Delta Lake no MinIO (bucket `gold`)

### Armazenamento

O projeto utiliza **MinIO** como camada de armazenamento object storage (S3-compatible) e **Delta Lake** como formato de dados:

- **MinIO**: Fornece armazenamento distribu√≠do compat√≠vel com S3, ideal para data lakes
- **Delta Lake**: Formato transacional sobre Parquet que oferece:
  - ACID transactions
  - Time travel (versionamento de dados)
  - Schema evolution
  - Upserts e deletes eficientes
  - Auditoria completa de mudan√ßas

## üõ†Ô∏è Stack Tecnol√≥gica

- **Apache Kafka**: Message broker para ingest√£o de dados em tempo real
- **Apache Spark**: Processamento distribu√≠do (Streaming e Batch)
- **Apache Airflow**: Orquestra√ß√£o e agendamento de pipelines
- **PostgreSQL**: Banco de dados para metadados do Airflow
- **MinIO**: Object Storage S3-compatible para armazenamento das camadas de dados
- **Delta Lake**: Formato de armazenamento transacional sobre Parquet para dados Bronze/Silver/Gold
- **Python**: Linguagem principal para desenvolvimento

## üöÄ Como Executar

### Pr√©-requisitos

- Docker e Docker Compose instalados
- M√≠nimo 8GB de RAM dispon√≠vel
- Portas dispon√≠veis: 5432, 8080, 8081, 8082, 8083, 9000, 9001, 9092, 2181

### Iniciar o Ambiente

```bash
# Subir todos os servi√ßos
docker-compose up -d

# Verificar status dos containers
docker-compose ps

# Ver logs de um servi√ßo espec√≠fico
docker-compose logs -f kafka
docker-compose logs -f spark-master
docker-compose logs -f airflow-webserver
```

### Acessar as Interfaces

O ambiente fornece v√°rias interfaces web para monitoramento e gerenciamento dos servi√ßos:

#### 1. Kafka UI
- **URL**: http://localhost:8080
- **Descri√ß√£o**: Interface web para gerenciar e monitorar o Apache Kafka
- **Funcionalidades**:
  - Visualizar t√≥picos, mensagens e consumidores
  - Criar e gerenciar t√≥picos
  - Inspecionar mensagens em tempo real
  - Ver m√©tricas de throughput e lat√™ncia
  - Gerenciar consumer groups
- **Uso**: √ötil para verificar se as transa√ß√µes est√£o sendo publicadas no t√≥pico `transactions_raw` e monitorar o fluxo de dados

#### 2. Spark Master UI
- **URL**: http://localhost:8081
- **Descri√ß√£o**: Interface web do Apache Spark Master para monitorar o cluster
- **Funcionalidades**:
  - Visualizar status do cluster Spark
  - Ver workers registrados e seus recursos (CPU, mem√≥ria)
  - Monitorar aplica√ß√µes Spark em execu√ß√£o
  - Ver hist√≥rico de jobs completados
  - Acessar logs de execu√ß√£o
  - Ver m√©tricas de performance
- **Uso**: Essencial para monitorar os jobs Spark (Bronze, Silver, Gold) e verificar o uso de recursos do cluster

#### 3. Spark Worker UI
- **URL**: http://localhost:8082
- **Descri√ß√£o**: Interface web do Apache Spark Worker para monitorar um n√≥ individual
- **Funcionalidades**:
  - Ver recursos dispon√≠veis do worker (cores, mem√≥ria)
  - Monitorar executors em execu√ß√£o
  - Visualizar logs do worker
  - Ver aplica√ß√µes alocadas neste worker
- **Uso**: √ötil para debug e monitoramento detalhado de um worker espec√≠fico do cluster

#### 4. MinIO Console
- **URL**: http://localhost:9001
- **Credenciais**:
  - Usu√°rio: `minioadmin`
  - Senha: `minioadmin`
- **Descri√ß√£o**: Interface web do MinIO para gerenciar buckets e objetos armazenados
- **Funcionalidades**:
  - Visualizar e gerenciar buckets (bronze, silver, gold)
  - Navegar pelos objetos armazenados
  - Upload/download de arquivos
  - Configurar pol√≠ticas de acesso
  - Ver m√©tricas de uso de armazenamento
  - Criar e gerenciar usu√°rios e pol√≠ticas
- **Uso**: Essencial para verificar os dados armazenados nas camadas Bronze, Silver e Gold, e gerenciar o armazenamento

#### 5. Airflow UI
- **URL**: http://localhost:8083
- **Credenciais**:
  - Usu√°rio: `admin`
  - Senha: `admin`
- **Descri√ß√£o**: Interface web do Apache Airflow para orquestra√ß√£o e monitoramento de pipelines
- **Funcionalidades**:
  - Visualizar e gerenciar DAGs (Directed Acyclic Graphs)
  - Monitorar execu√ß√µes de tarefas em tempo real
  - Ver hist√≥rico de execu√ß√µes e logs
  - Trigger manual de DAGs
  - Configurar schedules e depend√™ncias
  - Visualizar grafos de depend√™ncia entre tarefas
  - Acessar logs detalhados de cada task
- **Uso**: Interface principal para orquestrar todo o pipeline ETL, desde a gera√ß√£o de dados at√© as agrega√ß√µes finais

### Parar o Ambiente

```bash
# Parar todos os servi√ßos
docker-compose down

# Parar e remover volumes (limpar dados)
docker-compose down -v
```

## üìÅ Estrutura do Projeto

```
realtime-processing-etl/
‚îú‚îÄ‚îÄ Airflow/
‚îÇ   ‚îú‚îÄ‚îÄ dags/          # DAGs do Airflow
‚îÇ   ‚îú‚îÄ‚îÄ logs/          # Logs de execu√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ plugins/       # Plugins customizados
‚îú‚îÄ‚îÄ Kafka/
‚îÇ   ‚îî‚îÄ‚îÄ producers/     # Producers Python para gerar dados
‚îú‚îÄ‚îÄ Spark/
‚îÇ   ‚îî‚îÄ‚îÄ apps/          # Jobs Spark (Bronze, Silver, Gold)
‚îú‚îÄ‚îÄ Data/
‚îÇ   ‚îú‚îÄ‚îÄ Bronze/        # Dados brutos
‚îÇ   ‚îú‚îÄ‚îÄ Silver/        # Dados limpos e enriquecidos
‚îÇ   ‚îî‚îÄ‚îÄ Gold/          # Agrega√ß√µes e m√©tricas
‚îú‚îÄ‚îÄ docker-compose.yml # Configura√ß√£o dos servi√ßos
‚îî‚îÄ‚îÄ README.md          # Este arquivo
```

## üìä Fluxo de Dados

1. **Gera√ß√£o**: Producer Python gera transa√ß√µes fake (100-1000 transa√ß√µes/segundo)
2. **Ingest√£o**: Transa√ß√µes s√£o publicadas no t√≥pico Kafka `transactions_raw`
3. **Bronze**: Spark Streaming l√™ do Kafka e salva dados brutos em formato Delta Lake no MinIO (bucket `bronze`)
4. **Silver**: Spark Batch processa dados Bronze do MinIO, aplica valida√ß√µes e cria features, salvando em Delta Lake no MinIO (bucket `silver`)
5. **Gold**: Spark Batch agrega dados Silver do MinIO em m√©tricas de neg√≥cio, salvando em Delta Lake no MinIO (bucket `gold`)
6. **Orquestra√ß√£o**: Airflow coordena todo o pipeline de forma automatizada
7. **Armazenamento**: Todas as camadas (Bronze, Silver, Gold) s√£o armazenadas no MinIO usando Delta Lake para garantir ACID transactions e versionamento

## üîÑ Pr√≥ximos Passos

Consulte o arquivo `plan.md` para ver o planejamento completo de desenvolvimento em sprints.

## üìù Notas

- Este √© um projeto de demonstra√ß√£o/portf√≥lio
- Os dados gerados s√£o fict√≠cios usando a biblioteca Faker
- O ambiente est√° configurado para desenvolvimento local

## üîç Monitoramento e Verifica√ß√£o

### Verificando o Spark

Ap√≥s iniciar os servi√ßos, voc√™ pode verificar se o Spark est√° funcionando corretamente:

1. **Acessar a UI do Spark Master**: http://localhost:8081
   - Voc√™ deve ver o status do cluster e os workers registrados
   - Verifique se o worker aparece na lista de workers ativos

2. **Acessar a UI do Spark Worker**: http://localhost:8082
   - Mostra informa√ß√µes sobre o worker individual
   - Confirme que est√° conectado ao Master

3. **Verificar logs**:
   ```bash
   docker-compose logs spark-master
   docker-compose logs spark-worker
   ```

4. **Verificar status dos containers**:
   ```bash
   docker-compose ps spark-master spark-worker
   ```

O Spark Master e Worker devem estar com status "healthy" e o Worker deve aparecer registrado no Master.

### Quando Usar Cada Interface

- **Kafka UI**: Use para verificar se os dados est√£o sendo produzidos e consumidos corretamente, especialmente durante a Fase 2 (Gera√ß√£o de Dados)
- **Spark Master UI**: Use para monitorar jobs Spark em execu√ß√£o e verificar o status do cluster durante as Fases 3, 4 e 5 (Bronze, Silver, Gold)
- **Spark Worker UI**: Use para debug detalhado de problemas em um worker espec√≠fico
- **MinIO Console**: Use para verificar os dados armazenados nas camadas Bronze, Silver e Gold, navegar pelos buckets e verificar o uso de armazenamento
- **Airflow UI**: Use para orquestrar todo o pipeline, visualizar depend√™ncias entre tarefas e monitorar execu√ß√µes agendadas (Fase 6)

## üíæ Configura√ß√£o do MinIO e Delta Lake

### Acessando o MinIO via S3 API

O MinIO est√° configurado como um servi√ßo S3-compatible. Para acessar via c√≥digo Spark:

```python
# Configura√ß√£o para acessar MinIO como S3
spark.conf.set("spark.hadoop.fs.s3a.endpoint", "http://minio:9000")
spark.conf.set("spark.hadoop.fs.s3a.access.key", "minioadmin")
spark.conf.set("spark.hadoop.fs.s3a.secret.key", "minioadmin")
spark.conf.set("spark.hadoop.fs.s3a.path.style.access", "true")
spark.conf.set("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
```

### Buckets Dispon√≠veis

- **bronze**: Armazena dados brutos ingeridos do Kafka em formato Delta Lake
- **silver**: Armazena dados limpos e enriquecidos em formato Delta Lake
- **gold**: Armazena agrega√ß√µes e m√©tricas de neg√≥cio em formato Delta Lake

### Delta Lake

O projeto utiliza Delta Lake para todas as camadas de dados, oferecendo:

- **ACID Transactions**: Garantia de consist√™ncia dos dados
- **Time Travel**: Acesso a vers√µes hist√≥ricas dos dados
- **Schema Evolution**: Evolu√ß√£o autom√°tica do schema sem quebrar pipelines
- **Upserts e Deletes**: Opera√ß√µes eficientes de atualiza√ß√£o e exclus√£o
- **Auditoria**: Hist√≥rico completo de todas as mudan√ßas

### Exemplo de Uso no Spark

```python
# Configurar acesso ao MinIO como S3
spark.conf.set("spark.hadoop.fs.s3a.endpoint", "http://minio:9000")
spark.conf.set("spark.hadoop.fs.s3a.access.key", "minioadmin")
spark.conf.set("spark.hadoop.fs.s3a.secret.key", "minioadmin")
spark.conf.set("spark.hadoop.fs.s3a.path.style.access", "true")
spark.conf.set("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

# Ler dados Delta do MinIO
df = spark.read.format("delta").load("s3a://bronze/transactions")

# Escrever dados Delta no MinIO
df.write.format("delta").mode("overwrite").save("s3a://silver/transactions_cleaned")
```

### Depend√™ncias Necess√°rias

Para usar Delta Lake com Spark, √© necess√°rio incluir as seguintes bibliotecas ao submeter jobs:

- `io.delta:delta-spark_2.12:3.0.0` (ou vers√£o compat√≠vel com Spark 3.5.0)
- `org.apache.hadoop:hadoop-aws:3.3.4` (para suporte S3/MinIO)
- `com.amazonaws:aws-java-sdk-bundle:1.12.262` (para S3A filesystem)

Exemplo de submiss√£o:
```bash
spark-submit \
  --packages io.delta:delta-spark_2.12:3.0.0,org.apache.hadoop:hadoop-aws:3.3.4 \
  --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
  --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
  your_job.py
```

